# Machine Learning Essentials - TUM (WS 2025/2026)

Exercises and coursework for **Machine Learning Essentials (ED140024)** at the Technical University of Munich, taught by Prof. Dr. Julija Zavadlav.

## Course Overview

An introductory B.Sc. level course covering the foundations of machine learning and deep learning. Starting from probability theory and Python programming, the course introduces supervised, unsupervised, and reinforcement learning approaches applied to regression, classification, clustering, and dimensionality reduction tasks.

**Reference:** Bishop - *Pattern Recognition and Machine Learning*

## Topics Covered

| Exercise | Topic |
|----------|-------|
| 1 | Statistics and Gaussian Distributions |
| 2 | Regression (Polynomial Basis Functions, Least Squares) |
| 3 | Cross-Validation and Model Selection |
| 4 | Gradient Descent and Numerical Optimization |
| 7 | Neural Networks (Forward/Backprop from Scratch) |
| 9 | K-Nearest Neighbors and Decision Trees |
| 11 | Ensemble Methods (Random Forest, AdaBoost, Gradient Boosting) |
| 12 | Kernel Methods and Active Learning |
| 13 | Dimensionality Reduction (PCA) |

## What I Learned

- Probability theory fundamentals and how they underpin ML models
- Implementing regression models from scratch using polynomial basis functions and least squares
- Evaluating and selecting models through cross-validation and regularization
- Numerical optimization with gradient descent, momentum, and Adam
- Building neural networks from scratch (forward pass, backpropagation, ReLU activations)
- Non-parametric methods: KNN with distance weighting, decision trees with entropy/Gini splitting
- Ensemble learning: bagging, boosting, and out-of-bag evaluation
- Kernel methods (RBF kernels, kernel ridge regression) and active learning with uncertainty sampling
- Dimensionality reduction via PCA (standardization, eigendecomposition, variance explained)
